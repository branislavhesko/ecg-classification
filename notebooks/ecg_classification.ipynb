{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Machine learning spreads to day to day life with an immense speed. From medicine, industry to day to day life. It is helping or at least it should help people to make their life simpler. These are some areas, you probably even did not know that are heavily employing maching learning to improve their effectivness, costs or quality:\n",
    "\n",
    " - Traffic control - Intelligent traffic lights are varying green light time in order to minimize traffic jams.\n",
    " - Weather forecast - Many variables are influencing what the weather will be tommorrow, thus using past data to predict future state.\n",
    " - Medicine - With increasing amount of medical data, AI has to filter relevant information or help to do a better diagnosis.\n",
    " - Self-driving cars - Autopilot is one of the most known examples of AI in computer vision.\n",
    " - Fraud detection - Detection of suspicious behavior, traffic, credit card usement is widely used.\n",
    " - Agriculture - automatic harvest detection, quality control or plant disease prediction.\n",
    "\n",
    "\n",
    "# ECG classification\n",
    "\n",
    "In this short application, a simple classification of the ECG waveform is shown. There are a variety of models available for this task, but for deomnstration, state of the art transformer like model is used. Transformers are models composed of one or multiple transformer encoder layers consisting of an MLP part and Attention part. MLP is a simple linear, fully connected layer with expansion. GELU is used as activation layer. Compared to ReLU, GELU has nonzero activation for negative input.\n",
    "\n",
    "```python\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_channels, expansion=4):\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, input_channels * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_channels * expansion, input_channels)\n",
    "        ])\n",
    "```\n",
    "\n",
    "Attention is the key component of transformer models. Mathematically, it is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = \\frac{\\textbf{softmax(}QK^T\\textbf{)}V}{\\sqrt{d}},\n",
    "\\end{equation}\n",
    "where K, Q, V are keys, queries and values respectively. For self-attention, Q, K, V are identical. In general, one may imagine attention as values tensor being weighted by $QK^T$ multiplication. Thus, values are weighted by an matrix representing importance of each element. \n",
    "\n",
    "When combining attention and MLPs, layernorm has to be used to keep tensor values in a reasonable range. Additionally, using this approach, position information is lost as matrix multiplication and linear layers are position agnostic. Thus, at the input of each transformer encoder layer a learnable position encoding is added. Inspired by ResNet, skip connections are used. Skip connection allow residual learning, prevents gradient vanishing and were the first known way of how to train very deep models with hundreds of layers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

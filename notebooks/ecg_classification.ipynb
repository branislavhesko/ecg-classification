{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Machine learning spreads to day to day life with an immense speed. From medicine, industry to day to day life. It is helping or at least it should help people to make their life simpler. These are some areas, you probably even did not know that are heavily employing maching learning to improve their effectivness, costs or quality:\n",
    "\n",
    " - Traffic control - Intelligent traffic lights are varying green light time in order to minimize traffic jams.\n",
    " - Weather forecast - Many variables are influencing what the weather will be tommorrow, thus using past data to predict future state.\n",
    " - Medicine - With increasing amount of medical data, AI has to filter relevant information or help to do a better diagnosis.\n",
    " - Self-driving cars - Autopilot is one of the most known examples of AI in computer vision.\n",
    " - Fraud detection - Detection of suspicious behavior, traffic, credit card usement is widely used.\n",
    " - Agriculture - automatic harvest detection, quality control or plant disease prediction.\n",
    "\n",
    "\n",
    "# ECG classification\n",
    "\n",
    "In this short application, a simple classification of the ECG waveform is shown. A publicly available dataset [ECG Heartbeat Categorization Dataset](https://www.kaggle.com/datasets/shayanfazeli/heartbeat) is used to perform this task. \n",
    "\n",
    "There are a variety of models available for this task, but for deomnstration, state of the art transformer like model is used. Transformers are models composed of one or multiple transformer encoder layers consisting of an MLP part and Attention part. MLP is a simple linear, fully connected layer with expansion. GELU is used as activation layer. Compared to ReLU, GELU has nonzero activation for negative input.\n",
    "\n",
    "```python\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_channels, expansion=4):\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, input_channels * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_channels * expansion, input_channels)\n",
    "        ])\n",
    "```\n",
    "\n",
    "Attention is the key component of transformer models. Mathematically, it is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = \\frac{\\textbf{softmax(}QK^T\\textbf{)}V}{\\sqrt{d}},\n",
    "\\end{equation}\n",
    "where K, Q, V are keys, queries and values respectively. For self-attention, Q, K, V are identical. In general, one may imagine attention as values tensor being weighted by $QK^T$ multiplication. Thus, values are weighted by an matrix representing importance of each element. A simple pytorch implementation of self-attention layer may look like the following snippet:\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, attention_store=None):\n",
    "        super().__init__()\n",
    "        self.queries_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.values_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.keys_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.final_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3\n",
    "        keys = self.keys_projection(x)\n",
    "        values = self.values_projection(x)\n",
    "        queries = self.queries_projection(x)\n",
    "        keys = einops.rearrange(keys, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        queries = einops.rearrange(queries, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        values = einops.rearrange(values, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)\n",
    "        divider = sqrt(self.embed_size)\n",
    "        mh_out = torch.softmax(energy_term, -1)\n",
    "        out = torch.einsum('bihv, bvhd -> bihd ', mh_out / divider, values)\n",
    "        out = einops.rearrange(out, \"b n h e -> b n (h e)\")\n",
    "        return self.final_projection(out)\n",
    "```\n",
    "\n",
    "When combining attention and MLPs, layernorm has to be used to keep tensor values in a reasonable range. Additionally, using this approach, position information is lost as matrix multiplication and linear layers are position agnostic. Thus, at the input of each transformer encoder layer a learnable position encoding is added. Inspired by ResNet, skip connections are used. Skip connection allow residual learning, prevents gradient vanishing and were the first known way of how to train very deep models with hundreds of layers. A basic transformer encoder layer is afterwards implemented as:\n",
    "\n",
    "```python\n",
    "class TransformerEncoderLayer(torch.nn.Sequential):\n",
    "    def __init__(self, embed_size=768, expansion=4, num_heads=8):\n",
    "        super(TransformerEncoderLayer, self).__init__(\n",
    "            *[\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MultiHeadAttention(embed_size, num_heads)\n",
    "                ])),\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MLP(embed_size, expansion)\n",
    "                ]))\n",
    "            ]\n",
    "        )\n",
    "```\n",
    "\n",
    "After all encoder layers, the dimensionality of the problem is still preserved. The number of samples is still equal to the length of input sequence + 1. This additional item in input sequence length is given by classification token, which is added at the beginning. To reduce dimensionality, final classification layer is added. It first reduces dimensionality in sequence dimension by doing a simple mean. Finally, linear layers are used to reduce channel dimensions to the number of classes.\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Sequential):\n",
    "    def __init__(self, embed_size, num_classes):\n",
    "        super().__init__(*[\n",
    "            Reduce(\"b n e -> b e\", reduction=\"mean\"),\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, num_classes)\n",
    "        ])\n",
    "```\n",
    "\n",
    "Finally, combining all of these layers, a transformer is born.\n",
    "\n",
    "```python\n",
    "class ECGformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, signal_length, num_classes, input_channels, embed_size, num_heads, expansion) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList([TransformerEncoderLayer(\n",
    "            embed_size=embed_size, num_heads=num_heads, expansion=expansion) for _ in range(num_layers)])\n",
    "        self.classifier = Classifier(embed_size, num_classes)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(signal_length + 1, embed_size))\n",
    "        self.embedding = LinearEmbedding(input_channels, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            embedded = layer(embedded + self.positional_encoding)\n",
    "\n",
    "        return self.classifier(embedded)\n",
    "```\n",
    "\n",
    "\n",
    "This model will serve as a basic model for training. The input sequence length is 186, with classsification token, length becomes 187 samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "No special training is employed to do the actual training. The following settings are used:\n",
    "- learning rate - 0.0002\n",
    "- optimizer - Adam\n",
    "- batch size - 64.\n",
    "- augmentations - None\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

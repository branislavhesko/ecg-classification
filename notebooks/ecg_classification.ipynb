{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from IPython.display import Video\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from ecg_tools.config import EcgConfig, Mode\n",
    "from ecg_tools.data_loader import DatasetConfig, get_data_loaders\n",
    "from ecg_tools.model import ECGformer\n",
    "from ecg_tools.train import ECGClassifierTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG classification\n",
    "\n",
    "Machine learning spreads to day to day life with an immense speed. From medicine, industry to day to day life. It is helping or at least it should help people to make their life simpler. These are some areas, you probably even did not know that are heavily employing maching learning to improve their effectivness, costs or quality:\n",
    "\n",
    " - Traffic control - Intelligent traffic lights are varying green light time in order to minimize traffic jams.\n",
    " - Weather forecast - Many variables are influencing what the weather will be tommorrow, thus using past data to predict future state.\n",
    " - Medicine - With increasing amount of medical data, AI has to filter relevant information or help to do a better diagnosis.\n",
    " - Self-driving cars - Autopilot is one of the most known examples of AI in computer vision.\n",
    " - Fraud detection - Detection of suspicious behavior, traffic, credit card usement is widely used.\n",
    " - Agriculture - automatic harvest detection, quality control or plant disease prediction.\n",
    "\n",
    "\n",
    "In this short articles followed by a short code a simple demonstration of artificial intelligence on a real world data is used. With a little knowledge, many tasks may be automated and this is an example of how to do it. Please note that this code is task agnostic, there is no preliminary assumptions about input data, therefore with a little effort on modyfying dataloader, many one dimensional signal tasks may be solved.\n",
    "\n",
    "##Â Terminology\n",
    "\n",
    "First, not every reader is familiar with machine learning, therefore, for easier understanding, a list of few terms is presented:\n",
    "\n",
    " - Machine learning - Scientific field, where program, application or method that leverage data for learning and making decisions.\n",
    " - Model (neural network) - A structure composed of one or multiple neural layers that are processing data in some form.\n",
    " - Training - Data is submitted to algorithm in order to improve model performance. Some optimalization has to be made.\n",
    " - Linear layer - Layer performing matrix multiplication of input data and layer weights. Weight meaning can be explained by how each sample is influenced by other samples. To further improve performance, bias term is added.\n",
    " - Activation layer - Layer applaying a function to data. Known activation layers: RELU, GELU, Sigmoid, Tanh. They allow a model to learn non-linear functions.\n",
    " - Normalization layer - Layer transforming data range to reasonable range of values or statistics. Known normalization layers: BatchNorm, LayerNorm, GroupNorm. For example, LayerNorm is represented as: $$x_{new} = \\frac{x - mean(x)}{var(x)}$$\n",
    " - Batch size - How many training samples are used for optimalization step / inference at once.\n",
    " - Optimizer - Algorithm which performs gradient descent based on model error (distance from ground truth).\n",
    " - Loss function - Function which calculates the distance between ground truth label and prediction.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In this short application, a simple classification of the ECG waveform is shown. A publicly available dataset [ECG Heartbeat Categorization Dataset](https://www.kaggle.com/datasets/shayanfazeli/heartbeat) is used to perform this task. Only arythmia dataset is used and it consists of 109446 samples divided into 5 categories.\n",
    "\n",
    "![Categories](../assets/Screenshot%20from%202022-06-09%2022-53-30.png)\n",
    "\n",
    "\n",
    "\n",
    "First, let's do some exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60344/2096334936.py:15: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 5\n",
    "config = EcgConfig()\n",
    "data_loaders = get_data_loaders(EcgConfig().dataset)\n",
    "\n",
    "plt.figure(1, dpi=300)\n",
    "labels = []\n",
    "\n",
    "for idx, data in enumerate(data_loaders[Mode.train]):\n",
    "    if idx == NUM_EXAMPLES:\n",
    "        break\n",
    "    plt.plot(data[0][0, ...].squeeze().numpy())\n",
    "    labels.append(int(data[1][0, ...].numpy()))\n",
    "    \n",
    "plt.legend([f\"ECG: {label}\" for label in labels])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Class')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = EcgConfig()\n",
    "data_loaders = get_data_loaders(EcgConfig().dataset)\n",
    "classes = [\"N\", \"S\", \"V\", \"F\", \"Q\"]\n",
    "labels = {\n",
    "    idx: 0 for idx in range(len(classes))\n",
    "}\n",
    "\n",
    "for data in itertools.chain(*list(data_loaders.values())):\n",
    "    for label in data[1].numpy():\n",
    "        labels[int(label)] += 1\n",
    "\n",
    "plt.bar(range(len(classes)), list(labels.values()), tick_label=[f\"{k}: {v}\" for k, v in zip(classes, list(labels.values()))])\n",
    "plt.title(\"Class frequency for ECG classification task\")\n",
    "plt.ylabel(\"Frequency [samples]\")\n",
    "plt.xlabel(\"Class\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "Most of the times, the dataset size is limited. This is mainly caused by expensive manual labeling or inability of other training data. To tackle this type of problems, augmentations are often used.\n",
    "Augmentations are simple input dataset tranforms, which are sligthly modyfying data, such as it's annotation (meaning) remains known. It is also the case for problems like segmentation that the annotation is also modyfied. Augmentations help model to improve its generalization, prevent overfitting and artificially enlarge the training set. Therefore, augmentations are crucial part for training. In this work, two augmentations are used:\n",
    " - Random noise, which adds random uniform noise to the ECG signal.\n",
    " - Random shift, which shifts the signal by a random interval. To ensure the same signal length, signal is rolled (values from the end of the signal are moved to the beginning).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "There are a variety of models available for this task, but for demonstration, state of the art transformer like model is used. Transformers are models composed of one or multiple transformer encoder layers consisting of an MLP part and Attention part. MLP is a simple linear, fully connected layer with expansion. GELU is used as activation layer. Compared to ReLU, GELU has nonzero activation for negative input.\n",
    "\n",
    "```python\n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_channels, expansion=4):\n",
    "        super().__init__(*[\n",
    "            nn.Linear(input_channels, input_channels * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_channels * expansion, input_channels)\n",
    "        ])\n",
    "```\n",
    "\n",
    "Attention is the key component of transformer models. Mathematically, it is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = \\frac{\\textbf{softmax(}QK^T\\textbf{)}V}{\\sqrt{d}},\n",
    "\\end{equation}\n",
    "where K, Q, V are keys, queries and values respectively. For self-attention, Q, K, V are identical. In general, one may imagine attention as values tensor being weighted by $QK^T$ multiplication. Thus, values are weighted by an matrix representing importance of each element. A simple pytorch implementation of self-attention layer may look like the following snippet:\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, attention_store=None):\n",
    "        super().__init__()\n",
    "        self.queries_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.values_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.keys_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.final_projection = nn.Linear(embed_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3\n",
    "        keys = self.keys_projection(x)\n",
    "        values = self.values_projection(x)\n",
    "        queries = self.queries_projection(x)\n",
    "        keys = einops.rearrange(keys, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        queries = einops.rearrange(queries, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        values = einops.rearrange(values, \"b n (h e) -> b n h e\", h=self.num_heads)\n",
    "        energy_term = torch.einsum(\"bqhe, bkhe -> bqhk\", queries, keys)\n",
    "        divider = sqrt(self.embed_size)\n",
    "        mh_out = torch.softmax(energy_term, -1)\n",
    "        out = torch.einsum('bihv, bvhd -> bihd ', mh_out / divider, values)\n",
    "        out = einops.rearrange(out, \"b n h e -> b n (h e)\")\n",
    "        return self.final_projection(out)\n",
    "```\n",
    "\n",
    "When combining attention and MLPs, layernorm has to be used to keep tensor values in a reasonable range. Additionally, using this approach, position information is lost as matrix multiplication and linear layers are position agnostic. Thus, at the input of each transformer encoder layer a learnable position encoding is added. Inspired by ResNet, skip connections are used. Skip connection allow residual learning, prevents gradient vanishing and were the first known way of how to train very deep models with hundreds of layers. A basic transformer encoder layer is afterwards implemented as:\n",
    "\n",
    "```python\n",
    "class TransformerEncoderLayer(torch.nn.Sequential):\n",
    "    def __init__(self, embed_size=768, expansion=4, num_heads=8):\n",
    "        super(TransformerEncoderLayer, self).__init__(\n",
    "            *[\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MultiHeadAttention(embed_size, num_heads)\n",
    "                ])),\n",
    "                ResidualAdd(nn.Sequential(*[\n",
    "                    nn.LayerNorm(embed_size),\n",
    "                    MLP(embed_size, expansion)\n",
    "                ]))\n",
    "            ]\n",
    "        )\n",
    "```\n",
    "\n",
    "After all encoder layers, the dimensionality of the problem is still preserved. The number of samples is still equal to the length of input sequence + 1. This additional item in input sequence length is given by classification token, which is added at the beginning. To reduce dimensionality, final classification layer is added. It first reduces dimensionality in sequence dimension by doing a simple mean. Finally, linear layers are used to reduce channel dimensions to the number of classes.\n",
    "\n",
    "```python\n",
    "class Classifier(nn.Sequential):\n",
    "    def __init__(self, embed_size, num_classes):\n",
    "        super().__init__(*[\n",
    "            Reduce(\"b n e -> b e\", reduction=\"mean\"),\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, num_classes)\n",
    "        ])\n",
    "```\n",
    "\n",
    "Finally, combining all of these layers, a transformer is born.\n",
    "\n",
    "```python\n",
    "class ECGformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, signal_length, num_classes, input_channels, embed_size, num_heads, expansion) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList([TransformerEncoderLayer(\n",
    "            embed_size=embed_size, num_heads=num_heads, expansion=expansion) for _ in range(num_layers)])\n",
    "        self.classifier = Classifier(embed_size, num_classes)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(signal_length + 1, embed_size))\n",
    "        self.embedding = LinearEmbedding(input_channels, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            embedded = layer(embedded + self.positional_encoding)\n",
    "\n",
    "        return self.classifier(embedded)\n",
    "```\n",
    "\n",
    "\n",
    "This model will serve as a basic model for training. The input sequence length is 186, with classsification token, length becomes 187 samples. Please note, that we are adding positional encoding for each encoder layer. This approach was copied from [DETR]{https://arxiv.org/abs/2005.12872}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EcgConfig()\n",
    "trainer = ECGClassifierTrainer(config)\n",
    "\n",
    "train_confusion_matrix, eval_confusion_matrix = trainer.train()\n",
    "\n",
    "writer_train = cv2.VideoWriter(\"train.avi\", cv2.VideoWriter_fourcc(*\"XVID\"), 1, (train_confusion_matrix[0].shape[1], train_confusion_matrix[0].shape[0]))\n",
    "writer_eval = cv2.VideoWriter(\"eval.avi\", cv2.VideoWriter_fourcc(*\"XVID\"), 1, (eval_confusion_matrix[0].shape[1], eval_confusion_matrix[0].shape[0]))\n",
    "\n",
    "for cm in train_confusion_matrix:\n",
    "    writer_train.write(cm)\n",
    "    \n",
    "for cm in eval_confusion_matrix:\n",
    "    writer_eval.write(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "No special training procedure is employed to do the actual training. The following settings are used:\n",
    "- Learning rate - Determines the size of the step in the gradient direction. Too high learning rate leads to divergence, too low learning rate will slow down convergence. 0.0002 value is used for all experiments.\n",
    "- Optimizer - Performs gradient descent. Simple gradient descent may lead to unstabilities leading to harder convergence of the algorithm, therefore tricks like momentum, adaptive momentum are used. This works uses solely Adam (adaptive momentum) optimizer.\n",
    "- Batch size - How many samples are processed at once and used for gradient approximation. Lower values are worse approximation of the gradient of the entire dataset, but may be used as a good regularizer. This works uses 16, 32 or 64 sized batches.\n",
    "- Augmentations - Creating a populated real-world dataset is hard. Therefore, real datasets are small. Augmentations are used to artificially increase the size of the dataset by using various tricks on data, like noise, rotation, scaling etc. This work will show results under some augmentations.\n",
    "- Number of epochs - How many times is a single image passed through the network. This work is not designed to be SOTA, but rather educational, therefore 20 epochs are used.\n",
    "\n",
    "Why are these parameters important? It turns out, that for some applications, model is very sensitive to parameters, for example lower batch size may be the cause of model stagnating or higher learning rate may cause algorithm to overfit especially when the data quality is poor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Finally, this short article has been an attempt to introduce the reader into the amazing world of deep learning. The goal was to show that training a neural network for classification may not be a complicated task. \n",
    "A simple transformer network has been presented and only minor modifications would be needed to enlarge the model into 2D image processing model or even 3D model. Transformers are useful and powerful family of models that are processing inputs as sequences. They allow to combine multiple data sources like audio + images. Isn't it fascinating?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('colossus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6c35a7de3284dc6d49b39f9c645774a6148529021834bb0e43e7f6b73a9a42a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
